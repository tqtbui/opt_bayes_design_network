colnames(traces)[3] <- "iteration" #change the column name from id -> iteration
#order networks and model names
traces$network <- factor(traces$network,
levels = c("enron", "caltech"),
labels = c("Enron", "Caltech"),
ordered = TRUE)
traces$model_names <- factor(traces$model,
levels = c("locopt", "basse", "gane", "bintemp"),
labels = c("CNAR", "NS", "POW-DEG", "BNTAR"),
ordered = TRUE)
#setting limits for each pannel
limits <- data.frame(model_names = levels(traces$model_names),
min = c(0,0,0,0),
max = c(0.0015,5000,5,0.03))
dataC <- inner_join(traces, limits) %>% filter(value > min, value < max)
dataC$model_names <- factor(dataC$model_names,
levels = c("CNAR", "NS", "POW-DEG", "BNTAR"),
labels = c("CNAR", "NS", "POW-DEG", "BNTAR"),
ordered = TRUE)
#plot the results ----------------> FIGURE S1
ggline(data = dataC, x = "iteration", y = "value",
ylab = "Design Criterion \n", xlab = "\n Iteration",
plot_type = "l", facet.by = c("model_names", "network"),
size = 0.1, color = "run") +
scale_alpha_manual(values = 0.01) +
geom_vline(xintercept = 5000, colour = "black", linetype = "longdash") + #the reference line at 5000
facet_grid(model_names ~ network, scales="free") +
theme(legend.position="none") +
scale_color_manual(values = alpha(rep("black", 10), alpha = 0.2)) +
theme(strip.text = element_text(size = 14),
axis.title = element_text(size = 14, face = "bold"),
axis.text = element_text(size = 12))
#Specify the model and network. We will use the
model <- "bintemp" #available options are "locopt" (CNAR), "basse" (NS), "gane" (POW-DEG), and "bintemp" (BNTAR) models
network <- "enron" #other options are "caltech" and "umich". We choose the smallest, i.e., the "enron" network, for fastest results
library(igraph)
library(Matrix)
library(MASS)
library(parallel)
library(Rcpp)
sourceCpp("rcpp_help_funcs.cpp")
library(ggplot2)
library(ggpubr)
library(dplyr)
library(ggh4x)
load("funcs/mc_funcs.RData")
load("funcs/run_funcs.RData")
load(paste0("funcs/", model, "_funcs.RData"))
load("funcs/heuristics_funcs.RData")
load("funcs/neuralnet.RData")
load("funcs/cluster_funcs.RData")
load("funcs/bayesopt_funcs.RData")
#choose one of these algorithms to perform graph clustering
tmp <- louvain_wrapper(g)     #Louvain clustering
#load the corresponding network
if (network == "enron") {
g <- get(g_enron)
} else {
g <- get(paste0("g_fb_", network))
}
#load the corresponding network
if (network == "enron") {
g <- get("g_enron")
} else {
g <- get(paste0("g_fb_", network))
}
#choose one of these algorithms to perform graph clustering
tmp <- louvain_wrapper(g)     #Louvain clustering
#assign treatment and control using clustering info
assignment <- assign_mixed(tmp, clustered = TRUE)
#summarize all objects into the "clustering_compare.RData" file
clusters <- load("results/clustering_compare.RData")
load("funcs/mc_funcs.RData")
library(dplyr)
library(igraph)
library(chron)
#' Clustering summary
#'
#' @description This function summarizes the modularity, range of cluster sizes, and the running times for a clustering object in the "cluster_compare.RData" file
#' @param nm the name of the R object to be summarized
#' @return a list that contains the average modularity, range of cluster sizes, and the running times over all the runs in the clustering the object
cluster_summary <- function(nm) {
#get the clustering object and corresponding graph object
obj <- get(paste0(nm))
nm <- strsplit(nm, "_")[[1]]
if (nm[1] == "enron") {
g <- get(paste0("g_", nm[1]))
} else {
g <- get(paste0("g_fb_", nm[1]))
}
#place holder for results
result <- data.frame(modularity = numeric(),
range = numeric(),
times = character())
#calculate the modularity, cluster size range and running time for each run in the clustering object
cnt <- 0
for (i in 1:length(obj$times)) {
cnt <- cnt+1
tmp <- obj$clusters[,i]
result[cnt, "modularity"] <- modularity(g,tmp)
sizes <- sapply(1:length(unique(tmp)), function(i) {sum(tmp == i)})
result[cnt, "range"] <- max(sizes) - min(sizes)
result[cnt, "times"] <- obj$times[i]
}
#return
list(network = nm,
algo = ifelse(length(nm) == 3, paste0(nm[2:3], collapse = " "), nm[2]),
modularity = mean(result$modularity),
range = mean(result$range),
times = as.character(mean(times(result$times))))
}
#' Clustering summaries
#'
#' @description This function summarizes the modularity, range of cluster sizes, and the running times for a list of clustering objects in the "cluster_compare.RData" file
#' @param list_obj the list of names of R clustering objects to be summarized
#' @return a data frame that contains the summaries for each algorithm and network
cluster_summaries <- function(list_obj) {
#place holder for results
result <- data.frame(network = character(),
algo = character(),
modularity = numeric(),
range = numeric(),
times = character())
#summarize each object in the list
cnt <- 0
for (i in 1:length(list_obj)) {
cnt <- cnt+1
result[cnt,] <- cluster_summary(list_obj[i])
}
#return
result
}
#summarize all objects into the "clustering_compare.RData" file
#clusters <- load("results/clustering_compare.RData") #results from YOUR runs
clusters <- load("saved results/clustering_compare.RData") #results from runs in the paper
load("funcs/mc_funcs.RData")
#check if the functions work
cluster_summary("umich_reLDG")
#------------------> TABLE S1
cluster_summaries(clusters)
#Specify the model and network. We will use the
model <- "bintemp" #available options are "locopt" (CNAR), "basse" (NS), "gane" (POW-DEG), and "bintemp" (BNTAR) models
network <- "enron" #other options are "caltech" and "umich". We choose the smallest, i.e., the "enron" network, for fastest results
library(igraph)
library(Matrix)
library(MASS)
library(parallel)
library(Rcpp)
sourceCpp("rcpp_help_funcs.cpp")
library(ggplot2)
library(ggpubr)
library(dplyr)
library(ggh4x)
load("funcs/mc_funcs.RData")
load("funcs/run_funcs.RData")
load(paste0("funcs/", model, "_funcs.RData"))
load("funcs/heuristics_funcs.RData")
load("funcs/neuralnet.RData")
load("funcs/cluster_funcs.RData")
load("funcs/bayesopt_funcs.RData")
View(mass_clustering)
View(mass_assign)
#' @description This function cluster a graph using one algorithm for multiple times and save the results into a .RData file
#' @param g an igraph object
#' @param cluster_func the function to perform clustering
#' @param name_obj the name of the R object to be saved
#' @param nrun number of run repetition
#' @param name name of the algorithm to be saved in the object
#' @param file the name of the .RData file to save the result object. If set to NULL, the results will not be saved in an .RData file
#' @param seed seed number given at the beginning of the runs
#' @param ... parameters to be passed to the clustering function cluster_func()
#' @return an object that contains the information about the runs, including the clustering labels, the running times and the name of the algorithm used
mass_clustering <- function(g, cluster_func, name_obj,
nrun = 30, name = "balanced clustering",
file = "clustering.RData",
seed = 123456, ...) {
#initialize place holders
clusters <- c()
times <- c()
#set the randomization seed
if (!is.null(seed)) {set.seed(seed)}
for (i in 1:nrun) {
start <- Sys.time()
member <- do.call(cluster_func, args = list(g = g, ...)) #clustering
if (is.list(member)) {
member <- member$member
}
clusters <- cbind(clusters, member) #add in the results
end <- Sys.time()
times <- c(times, hms_span(start, end)) #add in the running times
if (!is.null(file)) {
eval(call("<-", as.name(name_obj),
list(clusters = clusters, times = times, algo = name)))
resave(list = name_obj, file = file) #save into the file
}
cat(i, "/", nrun, "\n")
}
list(clusters = clusters,
times = times,
algo = name)
}
save(louvain_wrapper, balanced_label_prop, reLDG, social_hash,
mass_clustering, resave,
assign_mixed, mass_assign,
file = "funcs/cluster_funcs.RData")
#if you want to run a graph clustering algorithm multiple times and save the results
tmp <- mass_clustering(g, cluster_func = reLDG,
name_obj = "mass_clustering",
nrun = 30, name = "reLDG",
file = NULL) #look at code file 2.4 to see the documentation of the function and what each parameter means
#Specify the model and network. We will use the
model <- "bintemp" #available options are "locopt" (CNAR), "basse" (NS), "gane" (POW-DEG), and "bintemp" (BNTAR) models
network <- "enron" #other options are "caltech" and "umich". We choose the smallest, i.e., the "enron" network, for fastest results
library(igraph)
library(Matrix)
library(MASS)
library(parallel)
library(Rcpp)
sourceCpp("rcpp_help_funcs.cpp")
library(ggplot2)
library(ggpubr)
library(dplyr)
library(ggh4x)
load("funcs/mc_funcs.RData")
load("funcs/run_funcs.RData")
load(paste0("funcs/", model, "_funcs.RData"))
load("funcs/heuristics_funcs.RData")
load("funcs/neuralnet.RData")
load("funcs/cluster_funcs.RData")
load("funcs/bayesopt_funcs.RData")
#load the corresponding network
if (network == "enron") {
g <- get("g_enron")
} else {
g <- get(paste0("g_fb_", network))
}
#choose one of these algorithms to perform graph clustering
tmp <- louvain_wrapper(g)     #Louvain clustering
tmp <- reLDG(g)               #restreaming linear deterministic greedy clustering
tmp <- social_hash(g)         #social hash clustering
tmp <- balanced_label_prop(g) #balanced label propagation clustering
#assign treatment and control using clustering info
assignment <- assign_mixed(tmp, clustered = TRUE)
#if you want to run a graph clustering algorithm multiple times and save the results
tmp <- mass_clustering(g, cluster_func = reLDG,
name_obj = "mass_clustering",
nrun = 30, name = "reLDG",
file = NULL) #look at code file 2.4 to see the documentation of the function and what each parameter means
View(tmp$clusters)
mean(times(tmp$times))
View(random_search)
#choose one of these algorithms to perform graph clustering
tmp <- random_search(g$N, crit.func = mc_func,
control.crit = list(g = g, trim = 0.05, mc.cores = 1), #set mc.cores > 1 for Mac or Linux
ndesign = 5000)
#get the relevant graph objects and function
g <- get(paste0("g_", network, "_", model)) #get the network
mc_func <- get(paste0("mc_", model))
#choose one of these algorithms to perform graph clustering
tmp <- random_search(g$N, crit.func = mc_func,
control.crit = list(g = g, trim = 0.05, mc.cores = 1), #set mc.cores > 1 for Mac or Linux
ndesign = 5000)
View(mass_assign)
#get the relevant graph objects and function
g <- get(paste0("g_", network, "_", model)) #get the network
mc_func <- get(paste0("mc_", model))
#generate set of designs produced by imbalanced cluster randomization
tmp <- mass_clustering(g,
cluster_func = louvain_wrapper,
name = "imbalanced cluster randomization",
nrun = 30, #number of runs
file = NULL)
#get the relevant graph objects and function
if (network == "enron") {
g <- get("g_enron")
} else {
g <- get(paste0("g_fb_", network))
}
mc_func <- get(paste0("mc_", model))
#generate set of designs produced by imbalanced cluster randomization
g <- get(paste0("g_", network, "_", model)) #get the network
tmp <- mass_clustering(g,
cluster_func = louvain_wrapper,
name = "imbalanced cluster randomization",
nrun = 30, #number of runs
file = NULL)
#get the relevant graph objects and function
if (network == "enron") {
g <- get("g_enron")
} else {
g <- get(paste0("g_fb_", network))
}
mc_func <- get(paste0("mc_", model))
#generate set of designs produced by imbalanced cluster randomization
tmp <- mass_clustering(g,
cluster_func = louvain_wrapper,
name = "imbalanced cluster randomization",
nrun = 30, #number of runs
file = NULL)
mass_assign(tmp, assign_func = assign_mixed,
control.assign = list(clustered = TRUE))
#evaluate the design criterion values of these designs
ret <- c()
for (i in 1:dim(tmp$best_designs)[2]) {
#evaluate the design according to a given design criteria
ret <- c(ret, do.call(mc_func, args = list(g = g,
assignment = tmp$best_designs[,i],
nsim = 50000, trim = 0.05))$value)
}
#evaluate the design criterion values of these designs
g <- get(paste0("g_", network, "_", model)) #get the network
ret <- c()
for (i in 1:dim(tmp$best_designs)[2]) {
#evaluate the design according to a given design criteria
ret <- c(ret, do.call(mc_func, args = list(g = g,
assignment = tmp$best_designs[,i],
nsim = 50000, trim = 0.05))$value)
}
tmp$best_vals <- ret
#----------------------->AVERAGE DESIGN CRITERION VALUES
mean(tmp$best_vals)
#----------------------->DESIGN CHARACTERISTICS
a <- tmp$best_designs[,1] #choose one design from the run, here I choose the first deisgn
#load the corresponding network
if (network == "enron") {
g <- get("g_enron")
} else {
g <- get(paste0("g_fb_", network))
}
A <- as_adjacency_matrix(g)
deg <- degree(g)
between <- betweenness(g)
close <- closeness(g)
#average difference in degree of treated vs controlled nodes
deg_diff <- mean(deg[a == 1], na.rm = TRUE) - mean(deg[a == 0], na.rm = TRUE)
#average difference in betweenness of treated vs controlled nodes
bet_diff <- mean(between[a == 1], na.rm = TRUE) - mean(between[a == 0], na.rm = TRUE)
#average difference in closeness of treated vs controlled nodes
clo_diff <- mean(close[a == 1], na.rm = TRUE) - mean(close[a == 0], na.rm = TRUE)
#percentage of higher than expected proportion of neighbors sharing same treatment assignment
per_nei_trt <- as.vector(A%*%as.matrix(a))/deg #expected proportion... for treated nodes
per_nei_trt[is.nan(per_nei_trt) | is.infinite(per_nei_trt)] <- 0
per_nei_ctl <- 1 - per_nei_trt #expected proportion... for controlled nodes
per_nei_ctl[deg == 0] <- 0
(sum(per_nei_trt[a==1] > per_trt) + sum(per_nei_ctl[a==0] > (1-per_trt)))/(sum(deg != 0))
#percentage of treated nodes
per_trt <- sum(a)/g$N
#percentage of treated nodes
per_trt <- sum(a)/g$N
(sum(per_nei_trt[a==1] > per_trt) + sum(per_nei_ctl[a==0] > (1-per_trt)))/(sum(deg != 0))
#percentage of treated nodes
per_trt <- sum(a)/g$N
per_trt
#----------------------->DESIGN CHARACTERISTICS
a <- tmp$best_designs[,1] #choose one design from the run, here I choose the first deisgn
a
N <- length(V(g))
#percentage of treated nodes
per_trt <- sum(a)/N
per_trt
#average difference in degree of treated vs controlled nodes
mean(deg[a == 1], na.rm = TRUE) - mean(deg[a == 0], na.rm = TRUE)
#average difference in betweenness of treated vs controlled nodes
mean(between[a == 1], na.rm = TRUE) - mean(between[a == 0], na.rm = TRUE)
#average difference in closeness of treated vs controlled nodes
mean(close[a == 1], na.rm = TRUE) - mean(close[a == 0], na.rm = TRUE)
#percentage of higher than expected proportion of neighbors sharing same treatment assignment
per_nei_trt <- as.vector(A%*%as.matrix(a))/deg
per_nei_trt[is.nan(per_nei_trt) | is.infinite(per_nei_trt)] <- 0
per_nei_ctl <- 1 - per_nei_trt
per_nei_ctl[deg == 0] <- 0
(sum(per_nei_trt[a==1] > per_trt) + sum(per_nei_ctl[a==0] > (1-per_trt)))/(sum(deg != 0))
library(igraph)
library(ggplot2)
library(ggpubr)
library(dplyr)
#' Flat the trace results
#'
#' @description This function get the results from the "mc_trace.RData" file and merge them into a single data frame
#' @param list_obj the list of "mc_trace" objects
#' @param list_names the names of the "mc_trace" objects
#' @return a data frame that contains the results of the "mc_trace" simulations
flat_trace <- function(list_obj, list_names) {
nobj <- length(list_obj)
#reshape
ret <- c()
for (i in 1:nobj) {
#extract the objects
obj <- list_obj[[i]]
nm <- list_names[i]
#reshape
obj <- as.data.frame(obj)
colnames(obj) <- 1:ncol(obj)
tmp <- reshape(obj, varying = 1:ncol(obj), v.names = "value",
timevar = "run", times = colnames(obj),
direction = "long", new.row.names = 1:(ncol(obj)*nrow(obj)))
nms <- strsplit(nm, split = "_")
tmp$network <- nms[[1]][1]
tmp$model <- nms[[1]][2]
#rbind the data frames
ret <- rbind(ret, tmp)
}
#return
ret
}
#load saved data
list_names <- load("results/mc_trace.RData") #YOUR results
#load saved data
#list_names <- load("results/mc_trace.RData") #YOUR results
list_names <- load("saved results/mc_trace.RData") #SAVED results
# list_names <- load("saved results/mc_trace.RData") #SAVED results
list_obj <- mget(list_names)
traces <- flat_trace(list_obj, list_names)
colnames(traces)[3] <- "iteration" #change the column name from id -> iteration
#order networks and model names
traces$network <- factor(traces$network,
levels = c("enron", "caltech"),
labels = c("Enron", "Caltech"),
ordered = TRUE)
traces$model_names <- factor(traces$model,
levels = c("locopt", "basse", "gane", "bintemp"),
labels = c("CNAR", "NS", "POW-DEG", "BNTAR"),
ordered = TRUE)
#setting y-axis limits for each pannel
limits <- data.frame(model_names = levels(traces$model_names),
min = c(0,0,0,0),
max = c(0.0015,5000,5,0.03))
dataC <- inner_join(traces, limits) %>% filter(value > min, value < max)
dataC$model_names <- factor(dataC$model_names,
levels = c("CNAR", "NS", "POW-DEG", "BNTAR"),
labels = c("CNAR", "NS", "POW-DEG", "BNTAR"),
ordered = TRUE)
#plot the results ----------------> FIGURE S1
#wait about 1-2 minutes for the plot to appear
ggline(data = dataC, x = "iteration", y = "value",
ylab = "Design Criterion \n", xlab = "\n Iteration",
plot_type = "l", facet.by = c("model_names", "network"),
size = 0.1, color = "run") +
scale_alpha_manual(values = 0.01) +
geom_vline(xintercept = 5000, colour = "black", linetype = "longdash") + #the reference line at 5000
facet_grid(model_names ~ network, scales="free") +
theme(legend.position="none") +
scale_color_manual(values = alpha(rep("black", 10), alpha = 0.2)) +
theme(strip.text = element_text(size = 14),
axis.title = element_text(size = 14, face = "bold"),
axis.text = element_text(size = 12))
library(dplyr)
library(igraph)
library(chron)
#' Clustering summary
#'
#' @description This function summarizes the modularity, range of cluster sizes, and the running times for a clustering object in the "cluster_compare.RData" file
#' @param nm the name of the R object to be summarized
#' @return a list that contains the average modularity, range of cluster sizes, and the running times over all the runs in the clustering the object
cluster_summary <- function(nm) {
#get the clustering object and corresponding graph object
obj <- get(paste0(nm))
nm <- strsplit(nm, "_")[[1]]
if (nm[1] == "enron") {
g <- get(paste0("g_", nm[1]))
} else {
g <- get(paste0("g_fb_", nm[1]))
}
#place holder for results
result <- data.frame(modularity = numeric(),
range = numeric(),
times = character())
#calculate the modularity, cluster size range and running time for each run in the clustering object
cnt <- 0
for (i in 1:length(obj$times)) {
cnt <- cnt+1
tmp <- obj$clusters[,i]
result[cnt, "modularity"] <- modularity(g,tmp)
sizes <- sapply(1:length(unique(tmp)), function(i) {sum(tmp == i)})
result[cnt, "range"] <- max(sizes) - min(sizes)
result[cnt, "times"] <- obj$times[i]
}
#return
list(network = nm,
algo = ifelse(length(nm) == 3, paste0(nm[2:3], collapse = " "), nm[2]),
modularity = mean(result$modularity),
range = mean(result$range),
times = as.character(mean(times(result$times))))
}
#' Clustering summaries
#'
#' @description This function summarizes the modularity, range of cluster sizes, and the running times for a list of clustering objects in the "cluster_compare.RData" file
#' @param list_obj the list of names of R clustering objects to be summarized
#' @return a data frame that contains the summaries for each algorithm and network
cluster_summaries <- function(list_obj) {
#place holder for results
result <- data.frame(network = character(),
algo = character(),
modularity = numeric(),
range = numeric(),
times = character())
#summarize each object in the list
cnt <- 0
for (i in 1:length(list_obj)) {
cnt <- cnt+1
result[cnt,] <- cluster_summary(list_obj[i])
}
#return
result
}
#summarize all objects into the "clustering_compare.RData" file
clusters <- load("results/clustering_compare.RData") #YOUR results
# clusters <- load("saved results/clustering_compare.RData") #SAVED results
load("funcs/mc_funcs.RData")
clusters <- load("saved results/clustering_compare.RData") #SAVED results
#------------------> TABLE S1
cluster_summaries(clusters)
